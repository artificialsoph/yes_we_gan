{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T23:44:52.571051Z",
     "start_time": "2018-07-09T23:44:51.015927Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Large amount of credit goes to:\n",
    "# https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py\n",
    "# which I've used as a reference for this implementation\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import keras\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T23:44:52.601805Z",
     "start_time": "2018-07-09T23:44:52.573289Z"
    }
   },
   "outputs": [],
   "source": [
    "img_rows = 32\n",
    "img_cols = 32\n",
    "channels = 3\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100\n",
    "\n",
    "# Following parameter and optimizer set as recommended in paper\n",
    "n_critic = 5\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T01:00:02.215068Z",
     "start_time": "2018-07-10T01:00:01.564092Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 2048)              206848    \n",
      "_________________________________________________________________\n",
      "reshape_3 (Reshape)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_9 (UpSampling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 32, 32, 64)        102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 32, 32, 3)         4803      \n",
      "=================================================================\n",
      "Total params: 593,859\n",
      "Trainable params: 593,347\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator and critic\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128 * 4 * 4, activation=\"relu\", input_dim=latent_dim),\n",
    "    keras.layers.Reshape((4, 4, 128)),\n",
    "    keras.layers.UpSampling2D(),\n",
    "    keras.layers.Conv2D(128, kernel_size=3, padding=\"same\", activation=\"elu\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.UpSampling2D(),\n",
    "    keras.layers.Conv2D(64, kernel_size=4, padding=\"same\", activation=\"elu\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.UpSampling2D(),\n",
    "    keras.layers.Conv2D(64, kernel_size=5, padding=\"same\", activation=\"elu\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.Conv2D(channels, kernel_size=5, padding=\"same\", activation=\"tanh\"),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "noise = keras.Input(shape=(latent_dim,))\n",
    "img = model(noise)\n",
    "\n",
    "generator = keras.Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T01:00:13.831465Z",
     "start_time": "2018-07-10T01:00:13.075070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 16)        448       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 8, 8, 32)          4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,385\n",
      "Trainable params: 99,937\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(16, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\", activation=\"elu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, strides=2, padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.LeakyReLU(alpha=.2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.LeakyReLU(alpha=.2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.LeakyReLU(alpha=.2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "img = keras.Input(shape=img_shape)\n",
    "validity = model(img)\n",
    "\n",
    "critic =  keras.Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T01:00:14.523601Z",
     "start_time": "2018-07-10T01:00:14.520134Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(keras.layers.merge._Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T01:00:15.400847Z",
     "start_time": "2018-07-10T01:00:15.396718Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T01:00:16.073816Z",
     "start_time": "2018-07-10T01:00:16.071129Z"
    }
   },
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T01:00:18.165131Z",
     "start_time": "2018-07-10T01:00:16.914715Z"
    }
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#       for the Critic\n",
    "#-------------------------------\n",
    "\n",
    "# Freeze generator's layers while training critic\n",
    "generator.trainable = False\n",
    "\n",
    "# Image input (real sample)\n",
    "real_img = keras.Input(shape=img_shape)\n",
    "\n",
    "# Noise input\n",
    "z_disc = keras.Input(shape=(latent_dim,))\n",
    "# Generate image based of noise (fake sample)\n",
    "fake_img = generator(z_disc)\n",
    "\n",
    "# Discriminator determines validity of the real and fake images\n",
    "fake = critic(fake_img)\n",
    "valid = critic(real_img)\n",
    "\n",
    "# Construct weighted average between real and fake images\n",
    "interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "# Determine validity of weighted sample\n",
    "validity_interpolated = critic(interpolated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T01:00:19.363208Z",
     "start_time": "2018-07-10T01:00:18.959719Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Python partial to provide loss function with additional\n",
    "# 'averaged_samples' argument\n",
    "partial_gp_loss = partial(gradient_penalty_loss,\n",
    "                  averaged_samples=interpolated_img)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "critic_model = keras.Model(inputs=[real_img, z_disc],\n",
    "                    outputs=[valid, fake, validity_interpolated])\n",
    "critic_model.compile(loss=[wasserstein_loss,\n",
    "                                      wasserstein_loss,\n",
    "                                      partial_gp_loss],\n",
    "                                optimizer=optimizer,\n",
    "                                loss_weights=[1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T01:00:20.177266Z",
     "start_time": "2018-07-10T01:00:19.552818Z"
    }
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#         for Generator\n",
    "#-------------------------------\n",
    "\n",
    "# For the generator we freeze the critic's layers\n",
    "critic.trainable = False\n",
    "generator.trainable = True\n",
    "\n",
    "# Sampled noise for input to generator\n",
    "z_gen = keras.Input(shape=(latent_dim,))\n",
    "# Generate images based of noise\n",
    "img = generator(z_gen)\n",
    "# Discriminator determines validity\n",
    "valid = critic(img)\n",
    "# Defines generator model\n",
    "generator_model = keras.Model(z_gen, valid)\n",
    "generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T01:00:20.369889Z",
     "start_time": "2018-07-10T01:00:20.360702Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "def train(epochs, batch_size, sample_interval=50):\n",
    "\n",
    "    shutil.rmtree(\"cifar_images\", ignore_errors=True)\n",
    "    os.makedirs(\"cifar_images\", exist_ok=True)\n",
    "    \n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "#     X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = -np.ones((batch_size, 1))\n",
    "    fake =  np.ones((batch_size, 1))\n",
    "    dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for _ in range(n_critic):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            # Sample generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            # Train the critic\n",
    "            d_loss = critic_model.train_on_batch([imgs, noise],\n",
    "                                                            [valid, fake, dummy])\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = generator_model.train_on_batch(noise, valid)\n",
    "        \n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "            sample_images(epoch)\n",
    "\n",
    "def sample_images(epoch):\n",
    "    # make a video with \n",
    "    # >ffmpeg -y -framerate 4 -pattern_type glob -i 'im_*.png' -pix_fmt yuv420p -vf scale=500:-1 output.mp4\n",
    "    \n",
    "    \n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + .5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c, sharex=True, sharey=True, frameon=False, figsize=(5,5))\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt,...], aspect=\"auto\")\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.tight_layout(h_pad=0, w_pad=0)\n",
    "    plt.suptitle(f'epoch: {epoch}', backgroundcolor=\"white\")\n",
    "    fig.savefig(f\"cifar_images/im_{epoch:05d}.png\", dpi=200)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-10T01:00:21.315Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 22.549532] [G loss: -0.085128]\n",
      "100 [D loss: -1.179994] [G loss: 2.338360]\n",
      "200 [D loss: -0.348127] [G loss: 3.588444]\n",
      "300 [D loss: -1.086858] [G loss: 5.133800]\n",
      "400 [D loss: -0.876872] [G loss: 4.884992]\n",
      "500 [D loss: -0.609653] [G loss: 4.347772]\n",
      "600 [D loss: -0.334168] [G loss: 2.548889]\n",
      "700 [D loss: -0.581471] [G loss: 1.469270]\n",
      "800 [D loss: -0.043264] [G loss: 0.461900]\n",
      "900 [D loss: -0.759276] [G loss: 0.855498]\n",
      "1000 [D loss: -0.437620] [G loss: 0.523317]\n",
      "1100 [D loss: -0.167538] [G loss: 0.852615]\n",
      "1200 [D loss: -0.268820] [G loss: 1.019385]\n",
      "1300 [D loss: 0.026647] [G loss: 0.963118]\n",
      "1400 [D loss: -0.514463] [G loss: 1.097671]\n",
      "1500 [D loss: -0.521503] [G loss: 0.904237]\n",
      "1600 [D loss: -0.245003] [G loss: 1.037712]\n",
      "1700 [D loss: -0.331149] [G loss: 1.138600]\n",
      "1800 [D loss: -0.096037] [G loss: 0.656050]\n",
      "1900 [D loss: -0.331608] [G loss: 0.802226]\n",
      "2000 [D loss: -0.213479] [G loss: 1.055718]\n",
      "2100 [D loss: -0.133520] [G loss: 1.378542]\n",
      "2200 [D loss: -0.270195] [G loss: 1.208834]\n",
      "2300 [D loss: -0.620956] [G loss: 1.045196]\n",
      "2400 [D loss: 0.129575] [G loss: 1.390137]\n",
      "2500 [D loss: -0.135021] [G loss: 1.240790]\n",
      "2600 [D loss: 0.004772] [G loss: 1.030175]\n",
      "2700 [D loss: -0.266619] [G loss: 0.971202]\n",
      "2800 [D loss: -0.424329] [G loss: 1.003967]\n",
      "2900 [D loss: -0.024668] [G loss: 1.173625]\n",
      "3000 [D loss: -0.520890] [G loss: 1.026607]\n",
      "3100 [D loss: -0.107200] [G loss: 0.964744]\n",
      "3200 [D loss: -0.680342] [G loss: 0.936440]\n",
      "3300 [D loss: -0.210980] [G loss: 0.851381]\n",
      "3400 [D loss: -0.247766] [G loss: 0.957628]\n",
      "3500 [D loss: -0.236647] [G loss: 0.850923]\n",
      "3600 [D loss: 0.066550] [G loss: 0.623489]\n",
      "3700 [D loss: 0.209552] [G loss: 0.754281]\n",
      "3800 [D loss: 0.060627] [G loss: 0.581525]\n",
      "3900 [D loss: -0.609523] [G loss: 0.925868]\n",
      "4000 [D loss: -0.008763] [G loss: 0.488506]\n",
      "4100 [D loss: -0.535225] [G loss: 0.741205]\n",
      "4200 [D loss: 0.049827] [G loss: 0.058111]\n",
      "4300 [D loss: 0.350899] [G loss: 0.551654]\n",
      "4400 [D loss: -0.315852] [G loss: 0.166304]\n",
      "4500 [D loss: 0.248353] [G loss: -0.100798]\n",
      "4600 [D loss: -0.050875] [G loss: 0.247401]\n",
      "4700 [D loss: -0.361693] [G loss: 0.350789]\n",
      "4800 [D loss: -0.134983] [G loss: -0.116569]\n",
      "4900 [D loss: -0.616744] [G loss: 0.273127]\n",
      "5000 [D loss: 0.226064] [G loss: -0.065241]\n",
      "5100 [D loss: -0.526015] [G loss: -0.140067]\n",
      "5200 [D loss: 0.266481] [G loss: 0.031739]\n",
      "5300 [D loss: -0.558354] [G loss: 0.125853]\n",
      "5400 [D loss: -0.305006] [G loss: -0.065003]\n",
      "5500 [D loss: 0.408729] [G loss: -0.154334]\n",
      "5600 [D loss: -0.102538] [G loss: 0.166054]\n",
      "5700 [D loss: -0.241311] [G loss: -0.124624]\n",
      "5800 [D loss: 0.299580] [G loss: 0.057404]\n",
      "5900 [D loss: -0.768846] [G loss: -0.770952]\n",
      "6000 [D loss: -0.001544] [G loss: -0.271936]\n",
      "6100 [D loss: -0.533041] [G loss: -0.739404]\n",
      "6200 [D loss: -0.136105] [G loss: -0.042228]\n",
      "6300 [D loss: 0.040623] [G loss: 0.165505]\n",
      "6400 [D loss: 0.047787] [G loss: -0.302787]\n",
      "6500 [D loss: 0.213082] [G loss: -0.548794]\n",
      "6600 [D loss: 0.288872] [G loss: -0.700627]\n",
      "6700 [D loss: -0.015675] [G loss: 0.002528]\n",
      "6800 [D loss: -0.139030] [G loss: -0.214295]\n",
      "6900 [D loss: -0.350725] [G loss: -0.345993]\n",
      "7000 [D loss: -0.072454] [G loss: -0.712036]\n",
      "7100 [D loss: 0.043221] [G loss: -0.671008]\n",
      "7200 [D loss: -0.214115] [G loss: -0.591233]\n",
      "7300 [D loss: -0.233655] [G loss: -0.201398]\n",
      "7400 [D loss: 0.201469] [G loss: -0.631313]\n",
      "7500 [D loss: -0.227395] [G loss: -0.375566]\n",
      "7600 [D loss: -0.172680] [G loss: -0.023697]\n",
      "7700 [D loss: -0.479409] [G loss: -0.444964]\n",
      "7800 [D loss: -0.375410] [G loss: -0.511276]\n",
      "7900 [D loss: 0.221936] [G loss: -0.704778]\n",
      "8000 [D loss: -0.408846] [G loss: -0.660522]\n",
      "8100 [D loss: 0.320127] [G loss: -0.465072]\n",
      "8200 [D loss: 0.174021] [G loss: -0.742079]\n",
      "8300 [D loss: -0.778626] [G loss: -0.753888]\n",
      "8400 [D loss: -0.093183] [G loss: -1.102416]\n",
      "8500 [D loss: 0.191112] [G loss: -0.649077]\n",
      "8600 [D loss: -0.026351] [G loss: -0.616052]\n",
      "8700 [D loss: -0.674502] [G loss: -0.963791]\n",
      "8800 [D loss: -0.724579] [G loss: -0.045035]\n",
      "8900 [D loss: -0.076216] [G loss: -0.532488]\n",
      "9000 [D loss: 0.063459] [G loss: -0.870621]\n",
      "9100 [D loss: 0.387855] [G loss: -0.486662]\n",
      "9200 [D loss: 0.030608] [G loss: -0.477158]\n",
      "9300 [D loss: 0.467665] [G loss: -0.107405]\n",
      "9400 [D loss: 0.347089] [G loss: -0.611046]\n",
      "9500 [D loss: -0.387824] [G loss: -0.547044]\n",
      "9600 [D loss: 0.026719] [G loss: -0.757916]\n",
      "9700 [D loss: 0.037767] [G loss: -0.471548]\n",
      "9800 [D loss: -0.226506] [G loss: -0.563996]\n",
      "9900 [D loss: -0.355703] [G loss: -0.235140]\n",
      "10000 [D loss: 0.288336] [G loss: -0.448006]\n",
      "10100 [D loss: -0.056120] [G loss: -0.909525]\n",
      "10200 [D loss: -0.172509] [G loss: -0.807343]\n",
      "10300 [D loss: 0.043324] [G loss: -1.272013]\n",
      "10400 [D loss: 0.010216] [G loss: -0.880323]\n",
      "10500 [D loss: 0.002082] [G loss: -1.220700]\n",
      "10600 [D loss: 0.318380] [G loss: -0.896243]\n",
      "10700 [D loss: 0.382312] [G loss: -1.242758]\n",
      "10800 [D loss: 0.494034] [G loss: -1.414497]\n",
      "10900 [D loss: 0.200103] [G loss: -0.850431]\n",
      "11000 [D loss: -0.426812] [G loss: -1.151603]\n",
      "11100 [D loss: -0.259237] [G loss: -0.822308]\n",
      "11200 [D loss: -0.040965] [G loss: -0.884168]\n",
      "11300 [D loss: -0.201027] [G loss: -1.292662]\n",
      "11400 [D loss: 0.033418] [G loss: -1.726441]\n",
      "11500 [D loss: -0.421266] [G loss: -1.478993]\n",
      "11600 [D loss: -0.139938] [G loss: -0.889107]\n",
      "11700 [D loss: -0.082028] [G loss: -1.164907]\n",
      "11800 [D loss: -0.323669] [G loss: -1.721127]\n",
      "11900 [D loss: 0.275806] [G loss: -1.490172]\n",
      "12000 [D loss: -0.077222] [G loss: -1.410839]\n",
      "12100 [D loss: -0.246161] [G loss: -0.857484]\n",
      "12200 [D loss: -0.072773] [G loss: -1.315830]\n",
      "12300 [D loss: 0.376365] [G loss: -1.010051]\n",
      "12400 [D loss: -0.624279] [G loss: -1.959089]\n",
      "12500 [D loss: 0.208496] [G loss: -2.030430]\n",
      "12600 [D loss: -0.293643] [G loss: -1.059283]\n",
      "12700 [D loss: -0.340479] [G loss: -1.792938]\n",
      "12800 [D loss: -0.099922] [G loss: -2.368900]\n",
      "12900 [D loss: 0.115681] [G loss: -1.767833]\n",
      "13000 [D loss: 0.011011] [G loss: -1.813550]\n",
      "13100 [D loss: -0.228066] [G loss: -1.200512]\n",
      "13200 [D loss: 0.168304] [G loss: -1.843969]\n",
      "13300 [D loss: -0.245911] [G loss: -2.159736]\n",
      "13400 [D loss: -0.303845] [G loss: -1.804023]\n",
      "13500 [D loss: -0.015136] [G loss: -2.111044]\n",
      "13600 [D loss: -0.290971] [G loss: -1.723764]\n",
      "13700 [D loss: -0.001758] [G loss: -1.398335]\n",
      "13800 [D loss: 0.428749] [G loss: -2.543548]\n",
      "13900 [D loss: 0.083530] [G loss: -2.434103]\n",
      "14000 [D loss: -0.622704] [G loss: -1.886202]\n",
      "14100 [D loss: -0.220315] [G loss: -2.392027]\n",
      "14200 [D loss: -0.067169] [G loss: -2.325277]\n",
      "14300 [D loss: -0.154733] [G loss: -1.949076]\n",
      "14400 [D loss: -0.207981] [G loss: -2.239646]\n",
      "14500 [D loss: -0.190998] [G loss: -2.197407]\n",
      "14600 [D loss: -0.022540] [G loss: -2.767002]\n",
      "14700 [D loss: -0.111112] [G loss: -2.610190]\n",
      "14800 [D loss: -0.732464] [G loss: -2.349562]\n",
      "14900 [D loss: -0.145576] [G loss: -2.362409]\n",
      "15000 [D loss: 0.367153] [G loss: -2.368599]\n",
      "15100 [D loss: -0.391298] [G loss: -2.404338]\n",
      "15200 [D loss: -0.322278] [G loss: -2.308553]\n",
      "15300 [D loss: -0.116129] [G loss: -3.196028]\n",
      "15400 [D loss: -0.391831] [G loss: -2.138487]\n",
      "15500 [D loss: -0.026771] [G loss: -2.640739]\n",
      "15600 [D loss: -0.355881] [G loss: -2.925911]\n",
      "15700 [D loss: -0.354270] [G loss: -2.324111]\n",
      "15800 [D loss: 0.262559] [G loss: -2.552758]\n",
      "15900 [D loss: -0.047071] [G loss: -2.671761]\n",
      "16000 [D loss: 0.278346] [G loss: -2.396409]\n",
      "16100 [D loss: -0.489185] [G loss: -2.548800]\n",
      "16200 [D loss: -0.103864] [G loss: -2.470320]\n",
      "16300 [D loss: -0.825953] [G loss: -2.148312]\n",
      "16400 [D loss: -0.085373] [G loss: -2.241836]\n",
      "16500 [D loss: -0.490422] [G loss: -2.723005]\n",
      "16600 [D loss: -0.365994] [G loss: -2.973475]\n",
      "16700 [D loss: -0.207039] [G loss: -3.091001]\n",
      "16800 [D loss: -0.369922] [G loss: -2.648545]\n",
      "16900 [D loss: 0.876275] [G loss: -2.950264]\n",
      "17000 [D loss: -0.468243] [G loss: -2.709149]\n",
      "17100 [D loss: 0.633005] [G loss: -2.432976]\n",
      "17200 [D loss: -0.386858] [G loss: -2.805661]\n",
      "17300 [D loss: 0.443949] [G loss: -3.237545]\n",
      "17400 [D loss: -0.221674] [G loss: -3.595037]\n",
      "17500 [D loss: -0.010409] [G loss: -3.253139]\n",
      "17600 [D loss: -0.062514] [G loss: -3.119811]\n",
      "17700 [D loss: 0.285720] [G loss: -3.229130]\n",
      "17800 [D loss: -0.720568] [G loss: -3.622504]\n",
      "17900 [D loss: -0.098630] [G loss: -3.048765]\n",
      "18000 [D loss: -0.193628] [G loss: -2.820994]\n",
      "18100 [D loss: -0.434617] [G loss: -3.656867]\n",
      "18200 [D loss: -0.016461] [G loss: -3.587483]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18300 [D loss: -0.417379] [G loss: -3.728946]\n",
      "18400 [D loss: -0.009635] [G loss: -3.070096]\n",
      "18500 [D loss: 0.343299] [G loss: -3.370257]\n",
      "18600 [D loss: -0.417341] [G loss: -3.487167]\n",
      "18700 [D loss: -0.160984] [G loss: -3.644227]\n",
      "18800 [D loss: 0.687193] [G loss: -2.934487]\n",
      "18900 [D loss: -0.032827] [G loss: -3.306953]\n",
      "19000 [D loss: -0.375450] [G loss: -3.747408]\n",
      "19100 [D loss: 0.561327] [G loss: -3.402855]\n",
      "19200 [D loss: -0.049396] [G loss: -2.947413]\n",
      "19300 [D loss: -0.023339] [G loss: -2.929947]\n",
      "19400 [D loss: -0.381016] [G loss: -2.465288]\n",
      "19500 [D loss: -0.491207] [G loss: -3.180260]\n",
      "19600 [D loss: -0.382616] [G loss: -3.309305]\n",
      "19700 [D loss: 0.112468] [G loss: -3.150632]\n",
      "19800 [D loss: 0.086436] [G loss: -3.324967]\n",
      "19900 [D loss: -0.077478] [G loss: -3.217413]\n",
      "20000 [D loss: 0.116025] [G loss: -3.367551]\n",
      "20100 [D loss: -0.119196] [G loss: -3.543710]\n",
      "20200 [D loss: 0.008402] [G loss: -3.135237]\n",
      "20300 [D loss: 0.149330] [G loss: -3.026728]\n",
      "20400 [D loss: -0.202665] [G loss: -3.087180]\n",
      "20500 [D loss: -0.152516] [G loss: -3.495648]\n",
      "20600 [D loss: 0.178672] [G loss: -3.681980]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=30000, batch_size=32, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T00:59:19.482382Z",
     "start_time": "2018-07-09T23:44:50.954Z"
    }
   },
   "outputs": [],
   "source": [
    "noise = np.random.normal(0, 1, (10, latent_dim))\n",
    "gen_imgs = generator.predict(noise)\n",
    "gen_imgs = 0.5 * gen_imgs + .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T00:59:19.483166Z",
     "start_time": "2018-07-09T23:44:50.957Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T00:59:19.484034Z",
     "start_time": "2018-07-09T23:44:50.958Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_imgs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-10T00:59:19.484817Z",
     "start_time": "2018-07-09T23:44:50.960Z"
    }
   },
   "outputs": [],
   "source": [
    "gen_imgs.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
