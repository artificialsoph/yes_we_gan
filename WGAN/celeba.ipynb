{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:45:56.675441Z",
     "start_time": "2018-07-12T00:45:53.667346Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# references\n",
    "# - https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py\n",
    "# - https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py\n",
    "# - https://github.com/LynnHo/WGAN-GP-DRAGAN-Celeba-Pytorch/blob/master/models_64x64.py\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import keras\n",
    "import keras_contrib\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import lycon\n",
    "from skimage import transform\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:46:54.653904Z",
     "start_time": "2018-07-12T00:46:54.625507Z"
    }
   },
   "outputs": [],
   "source": [
    "img_shape = [64, 64, 3]\n",
    "\n",
    "img_rows = img_shape[0]\n",
    "img_cols = img_shape[1]\n",
    "channels = img_shape[2]\n",
    "latent_dim = 100\n",
    "\n",
    "# Following parameter and optimizer set as recommended in paper\n",
    "n_critic = 5\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "optimizer = keras.optimizers.Adam(lr=lr, beta_1=.5, beta_2=.999)\n",
    "tnorm = keras.initializers.truncated_normal(stddev=.02)\n",
    "rnorm = keras.initializers.random_normal(stddev=.02)\n",
    "lrelu = lambda x: keras.activations.relu(x, alpha=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:46:47.324487Z",
     "start_time": "2018-07-12T00:45:57.425319Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes about 3.5min\n",
    "X_train = np.load(\"celeba.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:46:56.903362Z",
     "start_time": "2018-07-12T00:46:56.898948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202599, 64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:46:59.012825Z",
     "start_time": "2018-07-12T00:46:57.535856Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 8192)              819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8192)              32768     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 8, 8, 256)         3276800   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 16, 16, 128)       819200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 32, 32, 64)        204800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 64, 64, 3)         4803      \n",
      "=================================================================\n",
      "Total params: 5,159,363\n",
      "Trainable params: 5,142,083\n",
      "Non-trainable params: 17,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dim = 64\n",
    "DROP = 0\n",
    "\n",
    "# Build the generator\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(\n",
    "        dim * 8 * 4 * 4,\n",
    "        activation=\"relu\",\n",
    "        input_dim=latent_dim,\n",
    "        kernel_initializer=rnorm,\n",
    "        use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.Reshape((4, 4, dim * 8)),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Conv2DTranspose(\n",
    "        dim * 4,\n",
    "        kernel_size=5,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=rnorm,\n",
    "        use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Conv2DTranspose(\n",
    "        dim * 2,\n",
    "        kernel_size=5,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=rnorm,\n",
    "        use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Conv2DTranspose(\n",
    "        dim,\n",
    "        kernel_size=5,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=rnorm,\n",
    "        use_bias=False),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Conv2DTranspose(\n",
    "        channels,\n",
    "        kernel_size=5,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=rnorm,\n",
    "        activation=\"tanh\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "noise = keras.Input(shape=(latent_dim, ))\n",
    "img = model(noise)\n",
    "\n",
    "generator = keras.Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:46:59.111771Z",
     "start_time": "2018-07-12T00:46:59.108597Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def ln_func(x):\n",
    "    return tf.contrib.layers.layer_norm(x)\n",
    "\n",
    "layer_norm = lambda : keras.layers.Lambda(ln_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:47:03.773112Z",
     "start_time": "2018-07-12T00:46:59.203707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        4864      \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 128)       204928    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 256)         819456    \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 512)         3277312   \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 4,314,753\n",
      "Trainable params: 4,314,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build critic\n",
    "\n",
    "dim = 64\n",
    "DROP = 0.1\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=img_shape),\n",
    "    keras.layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size=5,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=lrelu,\n",
    "        kernel_initializer=tnorm,\n",
    "    ),\n",
    "    layer_norm(),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Conv2D(\n",
    "        dim * 2,\n",
    "        kernel_size=5,\n",
    "        strides=2,\n",
    "        input_shape=img_shape,\n",
    "        padding=\"same\",\n",
    "        activation=lrelu,\n",
    "        kernel_initializer=tnorm,\n",
    "    ),\n",
    "    layer_norm(),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Conv2D(\n",
    "        dim * 4,\n",
    "        kernel_size=5,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=lrelu,\n",
    "        kernel_initializer=tnorm,\n",
    "    ),\n",
    "    layer_norm(),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Conv2D(\n",
    "        dim * 8,\n",
    "        kernel_size=5,\n",
    "        strides=2,\n",
    "        padding=\"same\",\n",
    "        activation=lrelu,\n",
    "        kernel_initializer=tnorm,\n",
    "    ),\n",
    "    layer_norm(),\n",
    "    keras.layers.Dropout(DROP),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "img = keras.Input(shape=img_shape)\n",
    "validity = model(img)\n",
    "\n",
    "critic = keras.Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:47:03.878914Z",
     "start_time": "2018-07-12T00:47:03.875532Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(keras.layers.merge._Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((batch_size, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:47:03.974568Z",
     "start_time": "2018-07-12T00:47:03.970812Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:47:04.729165Z",
     "start_time": "2018-07-12T00:47:04.726507Z"
    }
   },
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:47:07.753553Z",
     "start_time": "2018-07-12T00:47:06.613272Z"
    }
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#       for the Critic\n",
    "#-------------------------------\n",
    "\n",
    "# Freeze generator's layers while training critic\n",
    "generator.trainable = False\n",
    "\n",
    "# Image input (real sample)\n",
    "real_img = keras.Input(shape=img_shape)\n",
    "\n",
    "# Noise input\n",
    "z_disc = keras.Input(shape=(latent_dim,))\n",
    "# Generate image based of noise (fake sample)\n",
    "fake_img = generator(z_disc)\n",
    "\n",
    "# Discriminator determines validity of the real and fake images\n",
    "fake = critic(fake_img)\n",
    "valid = critic(real_img)\n",
    "\n",
    "# Construct weighted average between real and fake images\n",
    "interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "# Determine validity of weighted sample\n",
    "validity_interpolated = critic(interpolated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:47:10.322417Z",
     "start_time": "2018-07-12T00:47:09.613306Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Python partial to provide loss function with additional\n",
    "# 'averaged_samples' argument\n",
    "partial_gp_loss = partial(gradient_penalty_loss,\n",
    "                  averaged_samples=interpolated_img)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "critic_model = keras.Model(inputs=[real_img, z_disc],\n",
    "                    outputs=[valid, fake, validity_interpolated])\n",
    "critic_model.compile(loss=[wasserstein_loss,\n",
    "                                      wasserstein_loss,\n",
    "                                      partial_gp_loss],\n",
    "                                optimizer=optimizer,\n",
    "                                loss_weights=[1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:47:14.278505Z",
     "start_time": "2018-07-12T00:47:13.560029Z"
    }
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#         for Generator\n",
    "#-------------------------------\n",
    "\n",
    "# For the generator we freeze the critic's layers\n",
    "critic.trainable = False\n",
    "generator.trainable = True\n",
    "\n",
    "# Sampled noise for input to generator\n",
    "z_gen = keras.Input(shape=(latent_dim,))\n",
    "# Generate images based of noise\n",
    "img = generator(z_gen)\n",
    "# Discriminator determines validity\n",
    "valid = critic(img)\n",
    "# Defines generator model\n",
    "generator_model = keras.Model(z_gen, valid)\n",
    "generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T00:47:42.563378Z",
     "start_time": "2018-07-12T00:47:42.554212Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "def train(steps, batch_size, sample_interval=50):\n",
    "    \n",
    "    d_loss_list = []\n",
    "    g_loss_list = []\n",
    "\n",
    "    shutil.rmtree(\"celeba_images\", ignore_errors=True)\n",
    "    os.makedirs(\"celeba_images\", exist_ok=True)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = -np.ones((batch_size, 1))\n",
    "    fake =  np.ones((batch_size, 1))\n",
    "    dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "    for step in tqdm(range(1,steps+1)):\n",
    "\n",
    "        for _ in range(n_critic):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            # Sample generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            # Train the critic\n",
    "            d_loss = critic_model.train_on_batch([imgs, noise],\n",
    "                                                            [valid, fake, dummy])\n",
    "            d_loss_list.append(d_loss)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = generator_model.train_on_batch(noise, valid)\n",
    "        g_loss_list.append(g_loss)\n",
    "        \n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if step % sample_interval == 0:\n",
    "            print_string = f\"step: {step}, D-W1: {d_loss[0]:g}, D-W2: {d_loss[1]:g}, D-GP: {d_loss[2]:g}, D: {d_loss[0]+d_loss[1]+10*d_loss[2]:g}, G: {g_loss:g}\"\n",
    "            print(print_string)\n",
    "            sample_images(print_string, step)\n",
    "\n",
    "def sample_images(print_string, step):\n",
    "    # make a video with \n",
    "    # >ffmpeg -y -framerate 4 -pattern_type glob -i 'im_*.png' -pix_fmt yuv420p -vf scale=500:-1 output.mp4\n",
    "    \n",
    "    \n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + .5\n",
    "\n",
    "    fig, axs = plt.subplots(r, c, sharex=True, sharey=True, frameon=False, figsize=(5,5))\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt,...], aspect=\"auto\", interpolation=\"spline16\")\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.tight_layout(h_pad=0, w_pad=0)\n",
    "    plt.suptitle(print_string, backgroundcolor=\"white\", fontsize=7)\n",
    "    fig.savefig(f\"celeba_images/im_{step:05d}.png\", dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-07-12T00:47:47.594Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0536972421645e4a3cce3d071f9861a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/keras/engine/training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "train(steps=30000, batch_size=batch_size, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
