{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:44.752299Z",
     "start_time": "2018-07-09T22:04:43.182207Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Large amount of credit goes to:\n",
    "# https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py\n",
    "# which I've used as a reference for this implementation\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import keras\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:44.781611Z",
     "start_time": "2018-07-09T22:04:44.754269Z"
    }
   },
   "outputs": [],
   "source": [
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "latent_dim = 100\n",
    "\n",
    "# Following parameter and optimizer set as recommended in paper\n",
    "n_critic = 5\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:45.754516Z",
     "start_time": "2018-07-09T22:04:44.783627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 1)         1025      \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the generator and critic\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128 * 7 * 7, activation=\"relu\", input_dim=latent_dim),\n",
    "    keras.layers.Reshape((7, 7, 128)),\n",
    "    keras.layers.UpSampling2D(),\n",
    "    keras.layers.Conv2D(128, kernel_size=4, padding=\"same\", activation=\"elu\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.UpSampling2D(),\n",
    "    keras.layers.Conv2D(64, kernel_size=4, padding=\"same\", activation=\"elu\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.Conv2D(channels, kernel_size=4, padding=\"same\", activation=\"tanh\"),\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "noise = keras.Input(shape=(latent_dim,))\n",
    "img = model(noise)\n",
    "\n",
    "generator = keras.Model(noise, img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:46.563493Z",
     "start_time": "2018-07-09T22:04:45.756626Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,097\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(16, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\", activation=\"elu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, strides=2, padding=\"same\"),\n",
    "    keras.layers.ZeroPadding2D(padding=((0,1),(0,1))),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.LeakyReLU(alpha=.2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.LeakyReLU(alpha=.2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Conv2D(128, kernel_size=3, strides=1, padding=\"same\"),\n",
    "    keras.layers.BatchNormalization(momentum=0.8),\n",
    "    keras.layers.LeakyReLU(alpha=.2),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "img = keras.Input(shape=img_shape)\n",
    "validity = model(img)\n",
    "\n",
    "critic =  keras.Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:46.569024Z",
     "start_time": "2018-07-09T22:04:46.565591Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomWeightedAverage(keras.layers.merge._Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:46.575756Z",
     "start_time": "2018-07-09T22:04:46.570911Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:46.582161Z",
     "start_time": "2018-07-09T22:04:46.577643Z"
    }
   },
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:47.798419Z",
     "start_time": "2018-07-09T22:04:46.584040Z"
    }
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#       for the Critic\n",
    "#-------------------------------\n",
    "\n",
    "# Freeze generator's layers while training critic\n",
    "generator.trainable = False\n",
    "\n",
    "# Image input (real sample)\n",
    "real_img = keras.Input(shape=img_shape)\n",
    "\n",
    "# Noise input\n",
    "z_disc = keras.Input(shape=(latent_dim,))\n",
    "# Generate image based of noise (fake sample)\n",
    "fake_img = generator(z_disc)\n",
    "\n",
    "# Discriminator determines validity of the real and fake images\n",
    "fake = critic(fake_img)\n",
    "valid = critic(real_img)\n",
    "\n",
    "# Construct weighted average between real and fake images\n",
    "interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "# Determine validity of weighted sample\n",
    "validity_interpolated = critic(interpolated_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:48.222636Z",
     "start_time": "2018-07-09T22:04:47.800130Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use Python partial to provide loss function with additional\n",
    "# 'averaged_samples' argument\n",
    "partial_gp_loss = partial(gradient_penalty_loss,\n",
    "                  averaged_samples=interpolated_img)\n",
    "partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "critic_model = keras.Model(inputs=[real_img, z_disc],\n",
    "                    outputs=[valid, fake, validity_interpolated])\n",
    "critic_model.compile(loss=[wasserstein_loss,\n",
    "                                      wasserstein_loss,\n",
    "                                      partial_gp_loss],\n",
    "                                optimizer=optimizer,\n",
    "                                loss_weights=[1, 1, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:48.743972Z",
     "start_time": "2018-07-09T22:04:48.224463Z"
    }
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Construct Computational Graph\n",
    "#         for Generator\n",
    "#-------------------------------\n",
    "\n",
    "# For the generator we freeze the critic's layers\n",
    "critic.trainable = False\n",
    "generator.trainable = True\n",
    "\n",
    "# Sampled noise for input to generator\n",
    "z_gen = keras.Input(shape=(latent_dim,))\n",
    "# Generate images based of noise\n",
    "img = generator(z_gen)\n",
    "# Discriminator determines validity\n",
    "valid = critic(img)\n",
    "# Defines generator model\n",
    "generator_model = keras.Model(z_gen, valid)\n",
    "generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T22:04:48.754893Z",
     "start_time": "2018-07-09T22:04:48.745716Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "def train(epochs, batch_size, sample_interval=50):\n",
    "\n",
    "    shutil.rmtree(\"images\", ignore_errors=True)\n",
    "    os.makedirs(f\"images\", exist_ok=True)\n",
    "    \n",
    "    # Load the dataset\n",
    "    (X_train, _), (_, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Rescale -1 to 1\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = -np.ones((batch_size, 1))\n",
    "    fake =  np.ones((batch_size, 1))\n",
    "    dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for _ in range(n_critic):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            # Sample generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n",
    "            # Train the critic\n",
    "            d_loss = critic_model.train_on_batch([imgs, noise],\n",
    "                                                            [valid, fake, dummy])\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "\n",
    "        g_loss = generator_model.train_on_batch(noise, valid)\n",
    "        \n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "            sample_images(epoch)\n",
    "\n",
    "def sample_images(epoch):\n",
    "    # make a video with \n",
    "    # >ffmpeg -framerate 4 -pattern_type glob -i mnist_*.png -pix_fmt yuv420p output.mp4\n",
    "    \n",
    "    \n",
    "    r, c = 5, 5\n",
    "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "\n",
    "    # Rescale images 0 - 1\n",
    "    gen_imgs = 0.5 * gen_imgs + 1\n",
    "\n",
    "    fig, axs = plt.subplots(r, c, sharex=True, sharey=True, frameon=False, figsize=(5,5))\n",
    "    cnt = 0\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray', aspect=\"auto\")\n",
    "            axs[i,j].axis('off')\n",
    "            cnt += 1\n",
    "    plt.tight_layout(h_pad=0, w_pad=0)\n",
    "    plt.suptitle(f'epoch: {epoch}', backgroundcolor=\"white\")\n",
    "    fig.savefig(f\"images/mnist_{epoch:05d}.png\", dpi=150)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T23:30:45.368474Z",
     "start_time": "2018-07-09T22:04:48.756692Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.6/site-packages/keras/engine/training.py:478: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 8.376334] [G loss: 0.348251]\n",
      "100 [D loss: -4.494257] [G loss: -0.427195]\n",
      "200 [D loss: -0.346024] [G loss: -0.547124]\n",
      "300 [D loss: -0.567543] [G loss: -1.540657]\n",
      "400 [D loss: -0.581732] [G loss: -1.897855]\n",
      "500 [D loss: -0.048570] [G loss: -1.934634]\n",
      "600 [D loss: -0.864462] [G loss: -0.926045]\n",
      "700 [D loss: -0.500787] [G loss: -0.586527]\n",
      "800 [D loss: -0.102128] [G loss: -1.145723]\n",
      "900 [D loss: -0.626080] [G loss: -1.844454]\n",
      "1000 [D loss: -1.185885] [G loss: -1.580364]\n",
      "1100 [D loss: -0.884991] [G loss: -1.315495]\n",
      "1200 [D loss: -1.657865] [G loss: -1.469141]\n",
      "1300 [D loss: -1.016559] [G loss: -1.626222]\n",
      "1400 [D loss: -1.173788] [G loss: 0.423203]\n",
      "1500 [D loss: -1.480434] [G loss: -0.033803]\n",
      "1600 [D loss: -2.240587] [G loss: 0.816050]\n",
      "1700 [D loss: -1.345939] [G loss: 0.523644]\n",
      "1800 [D loss: -1.398457] [G loss: 0.154413]\n",
      "1900 [D loss: -0.983535] [G loss: 0.916479]\n",
      "2000 [D loss: -0.712417] [G loss: 0.757155]\n",
      "2100 [D loss: -1.197576] [G loss: 1.079333]\n",
      "2200 [D loss: -1.535270] [G loss: 2.010621]\n",
      "2300 [D loss: -1.016370] [G loss: 1.277903]\n",
      "2400 [D loss: -0.361906] [G loss: 0.932776]\n",
      "2500 [D loss: -0.625736] [G loss: 0.744719]\n",
      "2600 [D loss: -1.189223] [G loss: 1.272700]\n",
      "2700 [D loss: 0.136207] [G loss: 0.490750]\n",
      "2800 [D loss: -0.807177] [G loss: 1.245327]\n",
      "2900 [D loss: -0.894076] [G loss: 1.875791]\n",
      "3000 [D loss: -0.738319] [G loss: 1.167630]\n",
      "3100 [D loss: 0.302308] [G loss: 0.973671]\n",
      "3200 [D loss: -0.850926] [G loss: 2.035757]\n",
      "3300 [D loss: -0.321773] [G loss: 1.090584]\n",
      "3400 [D loss: -1.565948] [G loss: 1.474080]\n",
      "3500 [D loss: -1.329742] [G loss: 0.636747]\n",
      "3600 [D loss: -1.132440] [G loss: 1.316862]\n",
      "3700 [D loss: -0.941833] [G loss: 0.936540]\n",
      "3800 [D loss: -1.319327] [G loss: 1.063059]\n",
      "3900 [D loss: -0.873499] [G loss: 0.964171]\n",
      "4000 [D loss: -1.288992] [G loss: 1.364377]\n",
      "4100 [D loss: -0.350147] [G loss: 1.779784]\n",
      "4200 [D loss: -0.526806] [G loss: 2.118395]\n",
      "4300 [D loss: -0.635423] [G loss: 2.332104]\n",
      "4400 [D loss: 0.397164] [G loss: 1.910188]\n",
      "4500 [D loss: -1.007766] [G loss: 1.492972]\n",
      "4600 [D loss: -0.127915] [G loss: 1.416285]\n",
      "4700 [D loss: -0.396201] [G loss: 1.934088]\n",
      "4800 [D loss: 0.356182] [G loss: 1.705818]\n",
      "4900 [D loss: -0.693830] [G loss: 1.837545]\n",
      "5000 [D loss: -0.623658] [G loss: 1.380587]\n",
      "5100 [D loss: -0.870638] [G loss: 2.490638]\n",
      "5200 [D loss: -0.061004] [G loss: 1.911099]\n",
      "5300 [D loss: -0.589220] [G loss: 1.572762]\n",
      "5400 [D loss: -0.388904] [G loss: 2.303768]\n",
      "5500 [D loss: -0.987523] [G loss: 1.473216]\n",
      "5600 [D loss: -0.636184] [G loss: 1.357951]\n",
      "5700 [D loss: -0.823057] [G loss: 0.778259]\n",
      "5800 [D loss: -1.369017] [G loss: 1.558771]\n",
      "5900 [D loss: -0.254447] [G loss: 1.637273]\n",
      "6000 [D loss: -0.214795] [G loss: 1.317675]\n",
      "6100 [D loss: -0.769135] [G loss: 2.323641]\n",
      "6200 [D loss: -0.026452] [G loss: 1.352727]\n",
      "6300 [D loss: -0.627416] [G loss: 1.071270]\n",
      "6400 [D loss: -0.738355] [G loss: 1.527826]\n",
      "6500 [D loss: -0.872818] [G loss: 2.070191]\n",
      "6600 [D loss: -0.341829] [G loss: 1.461557]\n",
      "6700 [D loss: -0.607964] [G loss: 1.265152]\n",
      "6800 [D loss: -1.583457] [G loss: 1.851821]\n",
      "6900 [D loss: 0.489259] [G loss: 2.352318]\n",
      "7000 [D loss: 0.203701] [G loss: 2.233788]\n",
      "7100 [D loss: 0.237278] [G loss: 1.776672]\n",
      "7200 [D loss: -1.098021] [G loss: 1.486372]\n",
      "7300 [D loss: -1.297915] [G loss: 2.092212]\n",
      "7400 [D loss: -0.635308] [G loss: 1.779927]\n",
      "7500 [D loss: -1.594169] [G loss: 1.622249]\n",
      "7600 [D loss: 0.029946] [G loss: 1.186023]\n",
      "7700 [D loss: -1.155205] [G loss: 1.686656]\n",
      "7800 [D loss: -0.861544] [G loss: 2.277656]\n",
      "7900 [D loss: -0.005393] [G loss: 2.180278]\n",
      "8000 [D loss: 0.286366] [G loss: 2.048403]\n",
      "8100 [D loss: -0.948049] [G loss: 1.464568]\n",
      "8200 [D loss: -0.722817] [G loss: 2.235375]\n",
      "8300 [D loss: -1.602000] [G loss: 1.478403]\n",
      "8400 [D loss: 0.491258] [G loss: 2.288739]\n",
      "8500 [D loss: -1.057562] [G loss: 1.638788]\n",
      "8600 [D loss: -0.289523] [G loss: 1.907953]\n",
      "8700 [D loss: -1.045576] [G loss: 1.996517]\n",
      "8800 [D loss: -0.143828] [G loss: 1.840057]\n",
      "8900 [D loss: -1.147874] [G loss: 1.701134]\n",
      "9000 [D loss: 0.061812] [G loss: 2.174913]\n",
      "9100 [D loss: -0.032810] [G loss: 1.812560]\n",
      "9200 [D loss: -0.371799] [G loss: 0.996397]\n",
      "9300 [D loss: -0.296122] [G loss: 2.174864]\n",
      "9400 [D loss: -0.773277] [G loss: 1.458921]\n",
      "9500 [D loss: -1.818303] [G loss: 1.777812]\n",
      "9600 [D loss: -1.661674] [G loss: 1.370122]\n",
      "9700 [D loss: -0.235114] [G loss: 1.639867]\n",
      "9800 [D loss: -0.160801] [G loss: 1.579824]\n",
      "9900 [D loss: -0.794700] [G loss: 0.823850]\n",
      "10000 [D loss: -0.144240] [G loss: 2.095155]\n",
      "10100 [D loss: -0.422503] [G loss: 1.702224]\n",
      "10200 [D loss: 0.034472] [G loss: 2.411020]\n",
      "10300 [D loss: -0.939077] [G loss: 2.475975]\n",
      "10400 [D loss: -0.494477] [G loss: 2.682417]\n",
      "10500 [D loss: -1.232684] [G loss: 1.554129]\n",
      "10600 [D loss: -0.919481] [G loss: 2.680121]\n",
      "10700 [D loss: -0.335346] [G loss: 2.267803]\n",
      "10800 [D loss: -0.713011] [G loss: 1.906835]\n",
      "10900 [D loss: -1.616635] [G loss: 2.136318]\n",
      "11000 [D loss: -0.461463] [G loss: 2.523029]\n",
      "11100 [D loss: -0.783460] [G loss: 2.538660]\n",
      "11200 [D loss: -0.175727] [G loss: 2.072358]\n",
      "11300 [D loss: -0.703701] [G loss: 2.351068]\n",
      "11400 [D loss: 0.395817] [G loss: 2.572535]\n",
      "11500 [D loss: 0.388677] [G loss: 1.499712]\n",
      "11600 [D loss: 0.209822] [G loss: 1.855982]\n",
      "11700 [D loss: -0.171631] [G loss: 2.316739]\n",
      "11800 [D loss: -0.941226] [G loss: 2.277700]\n",
      "11900 [D loss: -0.434918] [G loss: 2.379388]\n",
      "12000 [D loss: -0.619303] [G loss: 2.247656]\n",
      "12100 [D loss: -0.263362] [G loss: 2.352821]\n",
      "12200 [D loss: -1.138127] [G loss: 1.177938]\n",
      "12300 [D loss: -1.068084] [G loss: 1.916967]\n",
      "12400 [D loss: -0.793961] [G loss: 1.031548]\n",
      "12500 [D loss: -0.156479] [G loss: 1.270347]\n",
      "12600 [D loss: 0.221028] [G loss: 2.296691]\n",
      "12700 [D loss: -0.278923] [G loss: 2.385207]\n",
      "12800 [D loss: -0.533847] [G loss: 1.585634]\n",
      "12900 [D loss: -0.682523] [G loss: 1.161164]\n",
      "13000 [D loss: -0.648868] [G loss: 1.798077]\n",
      "13100 [D loss: -0.561514] [G loss: 2.423831]\n",
      "13200 [D loss: -1.273358] [G loss: 1.163862]\n",
      "13300 [D loss: -0.027585] [G loss: 1.547110]\n",
      "13400 [D loss: -0.543494] [G loss: 1.902841]\n",
      "13500 [D loss: -0.540777] [G loss: 2.411378]\n",
      "13600 [D loss: -1.356982] [G loss: 2.912284]\n",
      "13700 [D loss: 0.406148] [G loss: 1.972643]\n",
      "13800 [D loss: -0.079407] [G loss: 2.015678]\n",
      "13900 [D loss: -2.112600] [G loss: 1.026454]\n",
      "14000 [D loss: -0.332233] [G loss: 2.332272]\n",
      "14100 [D loss: -0.212991] [G loss: 2.042799]\n",
      "14200 [D loss: -0.000997] [G loss: 2.074991]\n",
      "14300 [D loss: -0.210367] [G loss: 2.361685]\n",
      "14400 [D loss: -0.317348] [G loss: 2.125673]\n",
      "14500 [D loss: 0.325775] [G loss: 1.957693]\n",
      "14600 [D loss: -0.459534] [G loss: 2.217907]\n",
      "14700 [D loss: -0.011199] [G loss: 2.332920]\n",
      "14800 [D loss: 0.569010] [G loss: 1.464021]\n",
      "14900 [D loss: 0.021438] [G loss: 2.031533]\n",
      "15000 [D loss: -0.565708] [G loss: 1.359136]\n",
      "15100 [D loss: -0.064289] [G loss: 1.635377]\n",
      "15200 [D loss: -0.031614] [G loss: 1.448584]\n",
      "15300 [D loss: -0.031308] [G loss: 2.007385]\n",
      "15400 [D loss: -0.864618] [G loss: 1.798478]\n",
      "15500 [D loss: -0.051805] [G loss: 2.493836]\n",
      "15600 [D loss: -0.441587] [G loss: 2.148541]\n",
      "15700 [D loss: -0.524517] [G loss: 2.095495]\n",
      "15800 [D loss: -0.285565] [G loss: 3.393274]\n",
      "15900 [D loss: -1.265244] [G loss: 1.847475]\n",
      "16000 [D loss: -1.292782] [G loss: 2.413818]\n",
      "16100 [D loss: -0.704934] [G loss: 2.240266]\n",
      "16200 [D loss: 0.213969] [G loss: 2.793176]\n",
      "16300 [D loss: -0.158127] [G loss: 2.320249]\n",
      "16400 [D loss: -0.249913] [G loss: 1.699582]\n",
      "16500 [D loss: 0.643621] [G loss: 1.982132]\n",
      "16600 [D loss: -0.668855] [G loss: 2.512334]\n",
      "16700 [D loss: -0.166276] [G loss: 2.030856]\n",
      "16800 [D loss: -0.699078] [G loss: 1.254406]\n",
      "16900 [D loss: -0.162926] [G loss: 1.563515]\n",
      "17000 [D loss: -0.218709] [G loss: 0.926679]\n",
      "17100 [D loss: -0.864317] [G loss: 2.703672]\n",
      "17200 [D loss: -0.528971] [G loss: 3.458370]\n",
      "17300 [D loss: -0.310796] [G loss: 2.434479]\n",
      "17400 [D loss: -0.712157] [G loss: 2.153775]\n",
      "17500 [D loss: -1.025741] [G loss: 2.204079]\n",
      "17600 [D loss: 0.146164] [G loss: 2.511631]\n",
      "17700 [D loss: 0.047148] [G loss: 1.693188]\n",
      "17800 [D loss: -0.237714] [G loss: 2.579563]\n",
      "17900 [D loss: -0.436758] [G loss: 1.293047]\n",
      "18000 [D loss: -0.123414] [G loss: 1.834232]\n",
      "18100 [D loss: -0.192034] [G loss: 1.836308]\n",
      "18200 [D loss: -0.768472] [G loss: 2.760181]\n",
      "18300 [D loss: 0.681688] [G loss: 2.256472]\n",
      "18400 [D loss: -0.219051] [G loss: 2.190076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18500 [D loss: -0.120024] [G loss: 1.457533]\n",
      "18600 [D loss: 0.160457] [G loss: 2.101882]\n",
      "18700 [D loss: 0.463108] [G loss: 2.499120]\n",
      "18800 [D loss: -0.467658] [G loss: 2.319057]\n",
      "18900 [D loss: -1.764443] [G loss: 1.769130]\n",
      "19000 [D loss: -0.143182] [G loss: 3.475824]\n",
      "19100 [D loss: -0.693433] [G loss: 3.007635]\n",
      "19200 [D loss: 0.071167] [G loss: 2.587565]\n",
      "19300 [D loss: -0.624237] [G loss: 2.982646]\n",
      "19400 [D loss: 0.010711] [G loss: 2.440885]\n",
      "19500 [D loss: 0.010004] [G loss: 1.869667]\n",
      "19600 [D loss: -0.401579] [G loss: 1.121923]\n",
      "19700 [D loss: -0.636725] [G loss: 2.218659]\n",
      "19800 [D loss: -0.410405] [G loss: 2.885381]\n",
      "19900 [D loss: 0.205634] [G loss: 1.868466]\n",
      "20000 [D loss: 0.293200] [G loss: 1.573007]\n",
      "20100 [D loss: -1.170002] [G loss: 1.313784]\n",
      "20200 [D loss: 0.299144] [G loss: 0.890134]\n",
      "20300 [D loss: 0.286147] [G loss: 2.271142]\n",
      "20400 [D loss: -0.759467] [G loss: 2.196149]\n",
      "20500 [D loss: -0.078327] [G loss: 0.771504]\n",
      "20600 [D loss: 0.419600] [G loss: 0.829716]\n",
      "20700 [D loss: 0.518876] [G loss: 1.677130]\n",
      "20800 [D loss: -0.324676] [G loss: 1.637615]\n",
      "20900 [D loss: 0.129889] [G loss: 1.845373]\n",
      "21000 [D loss: 0.564039] [G loss: 1.792349]\n",
      "21100 [D loss: -0.014444] [G loss: 2.192928]\n",
      "21200 [D loss: -0.893003] [G loss: 1.824201]\n",
      "21300 [D loss: -0.710769] [G loss: 1.946697]\n",
      "21400 [D loss: -0.332727] [G loss: 0.966309]\n",
      "21500 [D loss: -0.374472] [G loss: 1.026515]\n",
      "21600 [D loss: -0.803597] [G loss: 1.953204]\n",
      "21700 [D loss: 0.135553] [G loss: 1.856219]\n",
      "21800 [D loss: -1.166579] [G loss: 1.973700]\n",
      "21900 [D loss: -0.255576] [G loss: 2.459768]\n",
      "22000 [D loss: -0.532623] [G loss: 2.113740]\n",
      "22100 [D loss: 0.266360] [G loss: 1.936357]\n",
      "22200 [D loss: -0.303285] [G loss: 1.834860]\n",
      "22300 [D loss: 0.431963] [G loss: 2.452670]\n",
      "22400 [D loss: -0.517364] [G loss: 2.164147]\n",
      "22500 [D loss: -1.387577] [G loss: 1.741834]\n",
      "22600 [D loss: -0.852205] [G loss: 1.933214]\n",
      "22700 [D loss: 0.073486] [G loss: 3.317236]\n",
      "22800 [D loss: -0.606373] [G loss: 2.893602]\n",
      "22900 [D loss: 0.471322] [G loss: 3.307019]\n",
      "23000 [D loss: -0.887546] [G loss: 2.098001]\n",
      "23100 [D loss: 0.421688] [G loss: 2.184630]\n",
      "23200 [D loss: -0.431545] [G loss: 1.960056]\n",
      "23300 [D loss: -0.417471] [G loss: 1.810864]\n",
      "23400 [D loss: 0.244661] [G loss: 1.831239]\n",
      "23500 [D loss: 0.319325] [G loss: 1.244700]\n",
      "23600 [D loss: 0.382518] [G loss: 1.350853]\n",
      "23700 [D loss: 0.559447] [G loss: 1.867007]\n",
      "23800 [D loss: -0.976939] [G loss: 2.089553]\n",
      "23900 [D loss: -0.021711] [G loss: 1.158576]\n",
      "24000 [D loss: -0.561057] [G loss: 1.555217]\n",
      "24100 [D loss: 0.289004] [G loss: 1.531072]\n",
      "24200 [D loss: -0.773071] [G loss: 0.884684]\n",
      "24300 [D loss: -0.672948] [G loss: 2.341147]\n",
      "24400 [D loss: 0.537581] [G loss: 1.623533]\n",
      "24500 [D loss: -1.488089] [G loss: 1.366280]\n",
      "24600 [D loss: -0.273770] [G loss: 1.385333]\n",
      "24700 [D loss: 0.031938] [G loss: 1.342284]\n",
      "24800 [D loss: -0.142110] [G loss: 1.240670]\n",
      "24900 [D loss: 0.234612] [G loss: 0.680075]\n",
      "25000 [D loss: 0.321356] [G loss: 1.058624]\n",
      "25100 [D loss: 0.402172] [G loss: 0.839366]\n",
      "25200 [D loss: -0.146735] [G loss: 1.590404]\n",
      "25300 [D loss: -0.180071] [G loss: 1.700955]\n",
      "25400 [D loss: -0.313069] [G loss: 0.813438]\n",
      "25500 [D loss: -0.240673] [G loss: 1.017383]\n",
      "25600 [D loss: 0.541534] [G loss: -0.411817]\n",
      "25700 [D loss: -1.065898] [G loss: 1.269264]\n",
      "25800 [D loss: 0.280920] [G loss: 0.432391]\n",
      "25900 [D loss: -0.760489] [G loss: 0.780047]\n",
      "26000 [D loss: -1.146942] [G loss: 0.816007]\n",
      "26100 [D loss: -0.244819] [G loss: 0.875053]\n",
      "26200 [D loss: 0.138684] [G loss: 1.604106]\n",
      "26300 [D loss: -0.644582] [G loss: 1.268416]\n",
      "26400 [D loss: 0.987157] [G loss: 1.365983]\n",
      "26500 [D loss: -0.194517] [G loss: 2.235734]\n",
      "26600 [D loss: -0.565995] [G loss: 1.446983]\n",
      "26700 [D loss: -0.313845] [G loss: 0.776134]\n",
      "26800 [D loss: -0.155186] [G loss: 1.394007]\n",
      "26900 [D loss: 0.884731] [G loss: 0.654785]\n",
      "27000 [D loss: -0.274106] [G loss: 1.291562]\n",
      "27100 [D loss: -0.153138] [G loss: 0.440836]\n",
      "27200 [D loss: -0.366961] [G loss: 1.524611]\n",
      "27300 [D loss: 0.476967] [G loss: 0.446129]\n",
      "27400 [D loss: -0.653023] [G loss: 0.272393]\n",
      "27500 [D loss: 1.445542] [G loss: 0.775635]\n",
      "27600 [D loss: -0.157058] [G loss: 0.493286]\n",
      "27700 [D loss: -0.402709] [G loss: 0.226002]\n",
      "27800 [D loss: 0.080998] [G loss: 1.224333]\n",
      "27900 [D loss: -1.611369] [G loss: 0.997438]\n",
      "28000 [D loss: 1.183502] [G loss: 0.718838]\n",
      "28100 [D loss: -0.063255] [G loss: 1.049163]\n",
      "28200 [D loss: -0.041987] [G loss: 0.683636]\n",
      "28300 [D loss: -0.219192] [G loss: 0.134289]\n",
      "28400 [D loss: -0.952357] [G loss: 0.739239]\n",
      "28500 [D loss: 0.326741] [G loss: 0.849424]\n",
      "28600 [D loss: -0.055609] [G loss: 0.693990]\n",
      "28700 [D loss: 0.225523] [G loss: 1.203022]\n",
      "28800 [D loss: -0.463747] [G loss: 0.142328]\n",
      "28900 [D loss: -1.473856] [G loss: -0.112388]\n",
      "29000 [D loss: -0.586359] [G loss: 0.443020]\n",
      "29100 [D loss: 0.061480] [G loss: 0.570539]\n",
      "29200 [D loss: -0.918129] [G loss: 0.553103]\n",
      "29300 [D loss: -0.191578] [G loss: 1.364919]\n",
      "29400 [D loss: -0.193745] [G loss: 0.447541]\n",
      "29500 [D loss: -0.134892] [G loss: 0.740721]\n",
      "29600 [D loss: -0.829783] [G loss: -0.318093]\n",
      "29700 [D loss: -0.812755] [G loss: 0.747593]\n",
      "29800 [D loss: 0.410053] [G loss: 0.896741]\n",
      "29900 [D loss: -0.098967] [G loss: 0.602556]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=30000, batch_size=32, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
